# -*- coding: utf-8 -*-
"""Predictive Analytics.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cvhAFsk0f53F0Eq_CgEZCiu5Pzabbp5F

# **Proyek Predictive Analytics**
- **Nama:** Salsabila Mahiroh
- **Email:** Salsabilammm777@gmail.com
- **ID Dicoding:** salsabilammm

## **Business Understanding**

Problem Statement :
1. Fitur apa saja yang paling berpengaruh terhadap status pinjaman (loan_status)?
2. Bagaimana memilih model prediksi yang paling akurat untuk memprediksi apakah seseorang mengalami gagal bayar atau tidak dengan menggunakan beberapa model seperti KNN, Random Forest, dan XGBoost?

Goals atau tujuan berdasarkan problem statement :
1. Melakukan proses Exploratory Data Analysis (EDA) untuk mengetahui fitur atau variabel yang paling berkorelasi terhadap loan_status
2. Melakukan analisis dan membandingkan performa beberapa model prediksi seperti KNN, Random Forest, dan XGBoost untuk memilih model dengan kinerja terbaik dalam memprediksi status pinjaman berdasarkan riwayat calon peminjam.

## **Data Understanding**

Data Understanding digunakan untuk mendapatkan pemahaman dan informasi dari data yang digunakan dan menentukan kualitasnya. Kemudian data akan dilakukan explorasi sebelum melalui tahapan persiapan data (preparation)

- **Nama Dataset:** Credit Risk Dataset
- **Sumber Dataset:** Kaggle
- **Link Akses Dataset:** https://www.kaggle.com/datasets/laotse/credit-risk-dataset
- **Fitur :**

| No | Nama Fitur                   | Deskripsi                                                                |
| -- | ---------------------------- | ------------------------------------------------------------------------ |
| 1  | `person_age`                 | Usia pemohon pinjaman                                       |
| 2  | `person_income`              | Pendapatan tahunan pemohon                                               |
| 3  | `person_home_ownership`      | Status kepemilikan rumah                     |
| 4  | `person_emp_length`          | Lama masa kerja pemohon (dalam tahun)                                    |
| 5  | `loan_intent`                | Tujuan pinjaman  |
| 6  | `loan_grade`                 | Tingkat pinjaman                                    |
| 7  | `loan_amnt`                  | Jumlah pinjaman                                |
| 8  | `loan_int_rate`              | Suku bunga pinjaman                                        |
| 9  | `loan_status`                | Status pinjaman (fitur target): 0 = lancar, 1 = gagal bayar                   |
| 10 | `loan_percent_income`        | Persentase pinjaman terhadap pendapatan                                  |
| 11 | `cb_person_default_on_file`  | Riwayat default atau gagal bayar (YES/NO)                                      |
| 12 | `cb_person_cred_hist_length` | Durasi riwayat kredit (dalam tahun)                                      |

---
- **Dataset ini merupakan** dataset dengan metode klasifikasi biner, dataset ini digunakan untuk menganalisis risiko kredit dengan pendekatan klasifikasi. Tujuannya adalah memprediksi apakah seorang pemohon pinjaman memiliki risiko kredit "Tidak gagal bayar/non-default" (0) atau "Gagal Bayar/default" (1) berdasarkan data historis calon peminjam.

### **Import Library**
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
# %matplotlib inline
import seaborn as sns
from google.colab import files
import zipfile
import os
from sklearn.preprocessing import  OneHotEncoder
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score
from sklearn.metrics import roc_auc_score, RocCurveDisplay

"""### **Data Loading**

Pada bagian ini, file kaggle.json di upload untuk autentikasi ke kaggle, kemudian API key disalin ke direktori yang tepat. Dataset credit-risk-dataset diunduh dan diekstrak, kemudia dimuat kedalam variabel df dan membaca menggunakan pandas serta menampilkan lima baris pertama yang akan di tampilkan
"""

# upload fie json
files.upload()

# Setup API key agar bisa akses Kaggle Dataset
!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

# Download dataset dari Kaggle
!kaggle datasets download -d laotse/credit-risk-dataset

# Unzip file dataset
!unzip credit-risk-dataset.zip

# membaca file csv
df = pd.read_csv("credit_risk_dataset.csv")

# menampilkan 5 baris pertama
df.head(5)

"""### **Exploratory Data Analysis (EDA)**

#### **Exploratory Data Analysis - Memahami struktur & karakteristik data**:

Pada tahap ini EDA digunakan untuk, memahami struktur dan karakteristik data (data understanding) yang digunakan untuk mengenali tipe data, mendeteksi nilai yang hilang atau duplikat, melihat adanya outlier, dsb. Tujuannya sebagai dasar dalam pengambilan keputusan analisis selanjutnya.
"""

# cek informasi dataset
df.info()

"""**Insight :**
- Terdapat 4 kolom dengan tipe object, yaitu person_home_ownership, loan_intent, loan_grade, dan cb_person_default_on_file
- Terdapat 3 kolom numerik dengan tipe data float64 yaitu  person_emp_length, loan_int_rate, dan loan_percent_income  
- Terdapat 5 kolom numerik dengan tipe data int64 yaitu person_age, person_income, loan_amnt, loan_status, cb_person_cred_hist_length
- Terdapat fitur kategorikal biner yang direpresentasikan dengan integer yaitu loan_status sebagai fitur target, yang memiliki representasi 0 = tidak gagal bayar dan 1 = gagal bayar
"""

# Menampilkan statistik deskriptif untuk kolom numerik
df.describe()

# Menampilkan jumlah baris dan kolom dalam bentuk tuple
df.shape

"""**Insight :**

Dataset ini memiliki 32.581 data dan memiliki 12 kolom, 4 kolom kategori dan 8 kolom numerik (1 sebagai kolom target)
"""

# Menampilkan jumlah nilai unik di setiap kolom
df.nunique()

# Menampilkan missing values
df.isna().sum()

"""**Insight :**

Kolom person_emp_length dan loan_int_rate memiliki missing value, sehingga nantinya data akan dibersihkan pada tahap Data Preparation
"""

# Menampilkan data duplikat
print("Jumlah duplikasi: ", df.duplicated().sum())

"""**Insight :**

Data ini memiliki duplikasi data sebanyak 165, sehingga nantinya akan dibersihkan pada tahap Data Preparation dengan menghapus duplikasi data tersebut .

Pengecekan outlier pada fitur numerik akan di pilih terlebih dahulu, karena fitur loan_status adalah fitur target, maka fitur tersebut tidak perlu di ikut sertakan
"""

# cek data numerik yang memiliki outlier dengan boxplot
# Daftar fitur numerik yang relevan untuk pengecekan outlier
fitur_numerik = [
    'person_age',
    'person_income',
    'person_emp_length',
    'loan_amnt',
    'loan_int_rate',
    'loan_percent_income',
    'cb_person_cred_hist_length'
]

# Plot boxplot untuk masing-masing fitur numerik
plt.figure(figsize=(16, 12))
for i, col in enumerate(fitur_numerik):
    plt.subplot(3, 3, i+1)
    sns.boxplot(x=df[col], color='skyblue')
    plt.title(f'Boxplot: {col}')
    plt.tight_layout()

"""**Insight :**

Masing-masing fitur numerik memiliki outlier yang dapat mempengaruhi hasil prediksi atau pemodelan, sehingga outlier ini akan di tangani dan dibersihkan pada tahap Data Preparation

#### **Exploratory Data Analysis - Univariate Analysis**

Pada tahap Univariate Analysis, dilakukan analisis distribusi untuk fitur kategorikal dengan menghitung jumlah dan persentase kategori, serta divisualisasikan dengan diagram batang. Sedangkan untuk fitur numerik distribusi data divisualisasikan menggunakan histogram.
"""

# membagi fitur numerik dan kategori
fitur_numerik = ['person_age', 'person_income', 'person_emp_length', 'loan_amnt', 'loan_int_rate', 'loan_percent_income', 'cb_person_cred_hist_length', 'loan_status']
fitur_kategori = ['person_home_ownership', 'loan_intent', 'loan_grade', 'cb_person_default_on_file']

"""***Fitur Kategori***"""

# fitur person_home_ownership
feature = fitur_kategori[0]
count = df[feature].value_counts()
percent = 100 * df[feature].value_counts(normalize=True)
summary = pd.DataFrame({'jumlah sampel': count, 'persentase': percent.round(1)})
print(summary)
count.plot(kind='bar', title=feature)

"""**Insight :**

Berdasarkan informasi pada fitur person_home_ownership terdapat 4 kategori kepemilikan tempat tinggal peminjam, Rent (sewa) menduduki jumlah tertinggi, disusul mortgage (hipotek), own (milik sendiri), dan other (lainnya).
"""

# fitur loan_intent
feature = fitur_kategori[1]
count = df[feature].value_counts()
percent = 100 * df[feature].value_counts(normalize=True)
summary = pd.DataFrame({'jumlah sampel': count, 'persentase': percent.round(1)})
print(summary)
count.plot(kind='bar', title=feature)

"""**Insight :**

Berdasarkan informasi dari deskripsi variabel, fitur loan_intent terdiri dari 6 tujuan peminjaman, tujuan peminjam yang paling banyak jatuh kepada education, disusul medical, venture, personal, debtconsolidation, dan paling sedikit adalah homeimprovement
"""

# fitur loan_grade
feature = fitur_kategori[2]
count = df[feature].value_counts()
percent = 100 * df[feature].value_counts(normalize=True)
summary = pd.DataFrame({'jumlah sampel': count, 'persentase': percent.round(1)})
print(summary)
count.plot(kind='bar', title=feature)

"""**Insight :**

Terdapat 7 kategori pada fitur loan_grade, secara berurutan dari jumlah yang paling banyak yaitu dimulai dari grade A, B, C, D, E, F, hingga paling sedikit adalah G
"""

# fitur cb_person_default_on_file
feature = fitur_kategori[3]
count = df[feature].value_counts()
percent = 100 * df[feature].value_counts(normalize=True)
summary = pd.DataFrame({'jumlah sampel': count, 'persentase': percent.round(1)})
print(summary)
count.plot(kind='bar', title=feature)

"""**Insight :**

Berdasarkan deskripsi variabel, terdapat 2 kategori N dan Y. Variabel "N" memiliki jumlah sampel/presentase paling tinggi di banding "Y", artinya terdapat sedikit yang memiliki riwayat default (gagal bayar) .

**Insight :**

***Fitur Numerik***
"""

# melihat distribusi histogram fitur numerik
df.hist(bins=50, figsize=(20,15))
plt.show()

"""**Insight :**

Berdasarkan distribusi fitur numerik, khususnya pada fitur loan_status yang sebagai target, bahwa Mayoritas pinjaman berstatus lancar (0),dan hanya sebagian kecil yang gagal bayar (1). Artinya, sebagian besar peminjam dalam data ini berhasil membayar pinjamannya dengan lancar, sementara hanya sebagian kecil yang mengalami gagal bayar.

#### **Exploratory Data Analysis - Multivariate Analysis**

Pada tahap Multivariate Analysis, dilakukan analisis untuk memahami hubungan antar fitur dalam dataset, khususnya terhadap kolom target (loan_status). Untuk fitur kategorikal, analisis dilakukan dengan menggunakan diagram batang untuk menunjukkan distribusi jumlah atau persentase loan_status berdasarkan masing-masing kategori. Hal ini bertujuan untuk melihat apakah terdapat kategori tertentu yang lebih berisiko mengalami gagal bayar. Sementara itu, untuk fitur numerik, hubungan antar fitur divisualisasikan menggunakan pairplot, yang menampilkan scatter plot dan distribusi antar pasangan fitur. Selain itu, digunakan juga heatmap korelasi untuk menunjukkan tingkat hubungan (korelasi) antar fitur numerik dalam bentuk matriks warna, termasuk korelasinya terhadap loan_status.

***Fitur Kategori***
"""

# Daftar fitur kategorikal
fitur_kategori = ['person_home_ownership', 'loan_intent', 'loan_grade', 'cb_person_default_on_file']

# Crosstab dan visualisasi
for col in fitur_kategori:
    print(f"\n--- Distribusi Loan Status berdasarkan {col} ---")
    ct = pd.crosstab(df[col], df['loan_status'], normalize='index') * 100
    display(ct.round(1))
    # Ganti colormap dengan daftar warna spesifik untuk setiap kategori loan_status
    ct.plot(kind='bar', stacked=True, figsize=(8, 4), color=['steelblue', 'lightgray'])
    plt.title(f'Proporsi Loan Status berdasarkan {col}')
    plt.ylabel('Persentase (%)')
    plt.legend(title='Loan Status', labels=['Non-Default (0)', 'Default (1)'])
    plt.xticks(rotation=0)
    plt.tight_layout()
    plt.show()

"""**Insight :**
- Pada fitur person_home_ownership, presentase "gagal bayar" terhadap kepemilikian rumah sendiri (own) disusul oleh Mortgage, yang menunjukkan bahwa keduanya memiliki potensi untuk tidak gagal bayar. Sedangkan Rent dan Other memiliki risiko untuk gagal bayar. Hal ini fitur person_home_ownership memiliki pengaruh terhadap loan_status
- Pada fitur loan_intent, presentase "gagal bayar" terhadap Debtconsolidation, Homeimprovement, dan Medical lebih tinggi dibandingkan Venture, Education, dan Personal. Hal ini menunjukkan memiliki sedikit pengaruh terhadap loan_status
- Pada fitur loan_grade, semakin tingginya tingkat grade (A-G) semakin tingginya presentase terhadap "gagal bayar" yang artinya, memiliki potensi sebagai gagal bayar saat ini. Hal ini fitur loan_grade sangat berpengaruh terhadap loan_status
- Fitur cb_person_default_on_file memiliki pengaruh kuat terhadap loan_status. Persentase gagal bayar lebih tinggi pada kategori "Y", menunjukkan bahwa riwayat gagal bayar di masa lalu berkorelasi positif dengan potensi gagal bayar saat ini.
- Kesimpulan, fitur kategori memiliki peran penting terhadap loan_status

***Fitur Numerik***
"""

# Mengamati hubungan antar fitur numerik dengan fungsi pairplot()
sns.pairplot(df, diag_kind = 'kde')

"""**Insight :**

Berdasarkan visualisasi pairplot terlihat bahwa fitur loan_percent_income, loan_int_rate, dan person_income memiliki korelasi terhadap fitur loan_status. Sedangkan fitur lainnya memiliki korelasi yang lemah karena sebarannya tidak membentuk pola
"""

fitur_numerik = [
    'person_age', 'person_income', 'person_emp_length',
    'loan_amnt', 'loan_int_rate', 'loan_percent_income',
    'cb_person_cred_hist_length', 'loan_status'
]

plt.figure(figsize=(10, 8))
sns.heatmap(df[fitur_numerik].corr(), annot=True, cmap='coolwarm')
plt.title("Korelasi Antar Fitur Numerik")
plt.show()

"""**Insight :**
- korelasi loan_percent_income dengan loan_status adalah +0.38, yang artinya memiliki korelasi positif
- korelasi loan_int_rate dengan loan_status adalah +0.34, yang artinya memiliki korelasi positif
- korelasi loan_amnt dengan loan_status adalah +0.11, yang artinya memiliki korelasi sangat lemah
- korelasi person_income dengan loan_status adalah -0.14, yang artinya memiliki korelasi negatif lemah
- korelasi person_emp_length dengan loan_status adalah -0.082 , yang artinya memiliki korelasi sangat lemah
- korelasi person_age dengan loan_status adalah -0.022, yang artinya hampir tidak ada korelasi antar keduanya
- korelasi cb_person_cred_hist_length dengan loan_status adalah -0.016, yang artinya hampir tidak ada korelasi antar keduanya
- kesimpulan, fitur yang paling berkolerasi terhadap loan_status adalah loan_percent_income, loan_int_rate, dan person_income. Fitur numerik sisanya akan di tangani pada Data Preparation untuk menghapus fitur yang tidak memiliki korelasi dengan fitur target loan_status.

## **Data Preparation**

Setelah melakukan proses EDA dan menghasilkan insight untuk keputusan selanjutnya, maka kita akan mempersiapkan dan membersihkan data pada tahap Data Preparation agar bisa di proses untuk pemodelan dan evaluasi.

####***Drop Kolom***

Drop kolom yang tidak memiliki korelasi terhadap loan_status. Drop kolom fitur 'person_age', 'person_emp_length', 'loan_amnt', 'cb_person_cred_hist_length'
"""

# Daftar fitur yang ingin di-drop
drop_kolom = ['person_age', 'person_emp_length', 'loan_amnt', 'cb_person_cred_hist_length']

# Drop kolom dari DataFrame
df = df.drop(columns=drop_kolom)

# Cek hasil
print("Kolom setelah di-drop:", df.columns)
df.head()

"""####***Menangani missing value***"""

# Menampilkan missing values kembali setelah drop kolom
df.isna().sum()

"""Karena pada fitur loan_int_rate memiliki missing value sebanyak 3.116 dari 32.581 maka missing value ini akan di tangani dengan cara imputasi"""

# menangani missing value dengan imputasi
df['loan_int_rate'] = df['loan_int_rate'].fillna(df['loan_int_rate'].median())

# periksa kembali missing value setelah imputasi
print("Missing values setelah imputasi:")
print(df.isna().sum())

"""####***Menangani data duplikat***"""

# Mengecek jumlah data duplikat sebelum dihapus
print("Jumlah data duplikat sebelum dihapus:", df.duplicated().sum())

# Menghapus baris yang duplikat
df = df.drop_duplicates()

# Mengecek ulang jumlah data duplikat setelah dihapus
print("Jumlah data duplikat setelah dihapus:", df.duplicated().sum())

"""####***Menangani Outlier***"""

# Daftar kolom yang ingin ditangani outlier-nya
fitur_outlier = ['loan_percent_income', 'loan_int_rate', 'person_income']

# Tangani outlier dengan metode IQR
for col in fitur_outlier:
    Q1 = df[col].quantile(0.25)
    Q3 = df[col].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR

    # Tampilkan jumlah outlier sebelum dihapus
    outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]
    print(f"Jumlah outlier pada kolom {col}: {outliers.shape[0]}")

    # Hapus outlier
    df = df[(df[col] >= lower_bound) & (df[col] <= upper_bound)]

# Visualisasi boxplot setelah outlier dihapus
for col in fitur_outlier:
    plt.figure(figsize=(8, 5))
    sns.boxplot(x=df[col], color='steelblue')
    plt.title(f'Boxplot setelah menghapus outlier: {col}')
    plt.tight_layout()
    plt.show()

"""fitur loan_status merupakan fitur numerik dan fitur target, sehingga fitur tersebut tidak perlu ditangani dalam hal outlier

####***Encoding fitur kategori***
"""

# Fitur kategorikal yang akan di-encode
fitur_kategori = ['person_home_ownership', 'loan_intent', 'loan_grade', 'cb_person_default_on_file']

# Melakukan One-Hot Encoding pada fitur kategorikal dengan hasil numerik
df = pd.get_dummies(df, columns=fitur_kategori, drop_first=True, dtype=int)

# Menampilkan 5 baris pertama setelah encoding
print("DataFrame setelah One-Hot Encoding:")
display(df.head())

"""####***Train-test split***"""

# Pisahkan fitur (X) dan target (y)
X = df.drop('loan_status', axis=1)
y = df['loan_status']

# Split data menjadi data latih dan data uji (80:20)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# Tampilkan ukuran hasil split
print("Ukuran data training:", X_train.shape)
print("Ukuran data testing:", X_test.shape)

"""Pembagian data dilakukan dengan train-test split dengan porsi pembagian 80% untuk data latih (training) dan 20% untuk data uji (testing)

####***Standarisasi***
"""

# Menentukan fitur numerik
numerical_features = ['loan_percent_income', 'loan_int_rate', 'person_income']

# Normalisasi data numerik
scaler = StandardScaler()
scaler.fit(X_train[numerical_features])
X_train[numerical_features] = scaler.transform(X_train[numerical_features])
X_test[numerical_features] = scaler.transform(X_test[numerical_features])

"""## **Modelling**

####***K-Nearest Neighbor***

Model K-Nearest Neighbors dibangun menggunakan KNeighborsClassifier dari library scikit-learn dengan parameter n_neighbors=5, yang berarti model akan mempertimbangkan 5 tetangga terdekat untuk menentukan kelas dari sebuah data baru. Model ini kemudian dilatih (fit) menggunakan data pelatihan X_train dan y_train.
"""

# Modelling K-Nearest Neighbor
knn = KNeighborsClassifier(n_neighbors=5)
knn.fit(X_train, y_train)

"""####***Random Forest***

Model Random Forest dibangun menggunakan RandomForestClassifier dari scikit-learn. Dengan parameter n_estimators=100, berarti model menggunakan 100 pohon keputusan (decision trees). Parameter random_state=42 digunakan agar hasil model bersifat reproducible (konsisten setiap kali dijalankan). Model ini kemudian dilatih menggunakan data pelatihan X_train dan y_train.
"""

# Modelling Random Forest
rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(X_train, y_train)

"""####***XGboost***

Model XGBoost dibangun menggunakan XGBClassifier dengan parameter use_label_encoder=False, dan eval_metric='logloss' sebagai metrik evaluasi. Parameter random_state=42 digunakan untuk menjaga konsistensi hasil. Model ini kemudian dilatih menggunakan data pelatihan X_train dan y_train.
"""

# Modelling XGBoost
xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)
xgb.fit(X_train, y_train)

"""## **Evaluasi Model**

Setiap model (KNN, Random Forest, XGBoost) dievaluasi dengan metrik akurasi, f1-score, classification report, confusion matrix, dan ROC-AUC score. Visualisasi seperti kurva ROC dan confusion matrix digunakan untuk memahami kinerja model lebih lanjut.
"""

# Dictionary untuk menyimpan model
models = {
    'K-Nearest Neighbor': knn,
    'Random Forest': rf,
    'XGBoost': xgb
}

# Evaluasi masing-masing model
for name, model in models.items():
    print(f"--- Evaluasi Model: {name} ---")

    # Prediksi pada data uji
    y_pred = model.predict(X_test)

    # Classification report
    print("Classification Report:")
    print(classification_report(y_test, y_pred))

    # Confusion matrix
    cm = confusion_matrix(y_test, y_pred)
    plt.figure(figsize=(5, 4))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)
    plt.title(f'Confusion Matrix: {name}')
    plt.xlabel('Prediksi')
    plt.ylabel('Aktual')
    plt.show()

    # Akurasi dan F1-score
    accuracy = accuracy_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)
    print(f"Akurasi : {accuracy:.4f}")
    print(f"F1-score: {f1:.4f}")

    # ROC-AUC Score dan Kurva ROC
    if hasattr(model, "predict_proba"):
        y_proba = model.predict_proba(X_test)[:, 1]
        auc = roc_auc_score(y_test, y_proba)
        print(f"ROC-AUC Score: {auc:.4f}")
        RocCurveDisplay.from_predictions(y_test, y_proba)
        plt.title(f'ROC Curve: {name}')
        plt.show()

    print("-" * 30)

# Menyimpan metrik setiap model
model_names = []
accuracies = []
f1_scores = []
roc_aucs = []

for name, model in models.items():
    model_names.append(name)
    y_pred = model.predict(X_test)

    accuracy = accuracy_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)

    if hasattr(model, "predict_proba"):
        y_proba = model.predict_proba(X_test)[:, 1]
        auc = roc_auc_score(y_test, y_proba)
    else:
        auc = np.nan

    accuracies.append(accuracy)
    f1_scores.append(f1)
    roc_aucs.append(auc)

# Buat posisi bar chart
x = np.arange(len(model_names))
width = 0.25

fig, ax = plt.subplots(figsize=(10, 6))

# Bar chart akurasi, f1, dan roc-auc
bar1 = ax.bar(x - width, accuracies, width, label='Accuracy')
bar2 = ax.bar(x, f1_scores, width, label='F1 Score')
bar3 = ax.bar(x + width, roc_aucs, width, label='ROC-AUC')

# Label dan judul
ax.set_xlabel('Model')
ax.set_ylabel('Score')
ax.set_title('Perbandingan Performa Model')
ax.set_xticks(x)
ax.set_xticklabels(model_names)
ax.legend()
ax.legend(loc='lower center', bbox_to_anchor=(0.5, -0.2), ncol=3)

# Tambah nilai di atas bar
def add_values(bars):
    for bar in bars:
        height = bar.get_height()
        ax.annotate(f'{height:.3f}',
                    xy=(bar.get_x() + bar.get_width() / 2, height),
                    xytext=(0, 3),  # Offset label
                    textcoords="offset points",
                    ha='center', va='bottom')

add_values(bar1)
add_values(bar2)
add_values(bar3)

plt.ylim(0, 1)
plt.show()

# Ambil 1 sampel data dari X_test dan y_test
sample = X_test.iloc[:1].copy()
true_value = y_test.iloc[:1].values[0]

# Buat dictionary awal dengan nilai aktual
pred_dict = {'y_true': [true_value]}

# Melakukan prediksi dengan masing-masing model dan menyimpan hasilnya
for name, model in models.items():
    prediksi = model.predict(sample).round(1)
    pred_dict['prediksi_' + name.replace(" ", "_")] = prediksi

# Tampilkan hasil prediksi dalam bentuk DataFrame
hasil_prediksi = pd.DataFrame(pred_dict)
display(hasil_prediksi)

"""# **ðŸ” Interprestasi Hasil dan Keterkaitam dengan problem statment & tujuan bisnis**


â­**Problem statment 1** : Fitur apa saja yang paling berpengaruh terhadap status pinjaman (loan_status)?

**Goal 1** :  Melakukan proses Exploratory Data Analysis (EDA) untuk mengetahui fitur atau variabel yang paling berkorelasi terhadap loan_status

**Hasil & interprestasi** : Setelah melewati proses exploratory data - Multivariate analisis yaitu, untuk memahami hubungan antara dua atau lebih variabel pada data, dapat diperoleh bahwa fitur seperti ***person_income, person_home_ownership, loan_intent, loan_grade, loan_intent, loan_grade, loan_int_rate, loan_percent_income, cb_person_default_file*** memberikan dan menunjukan hubungan atau korelasi terhadap loan_status. Setelah mengetahui fitur yang berkolerasi terhadap loan_status, maka selanjutnya akan diproses dalam tahap Data Preparation untuk memilih dan menghapus fitur yang tidak diperlukan (tidak memiliki korelasi), sehingga dapat dilanjutkan ke tahap pemodelan untuk menghasilkan suatu prediksi.

â­**Problem statment 2** : Bagaimana memilih model prediksi yang paling akurat untuk memprediksi apakah seseorang mengalami gagal bayar atau tidak dengan menggunakan beberapa model seperti KNN, Random Forest, dan XGBoost?

**Goal 2** :
Melakukan analisis dan membandingkan performa beberapa model prediksi seperti KNN, Random Forest, dan XGBoost untuk memilih model dengan kinerja terbaik dalam memprediksi status pinjaman berdasarkan riwayat calon peminjam.

**Hasil & interprestasi**: Setelah melakukan modelling dan melakukan evaluasi model prediksi menggunakan clasification report, confussion matrix, dan ROC-AUC diperoleh hasil :

| Model              | Accuracy | F1 Score | ROC-AUC |
| ------------------ | -------- | -------- | ------- |
| K-Nearest Neighbor | 0.893    | 0.715    | 0.871   |
| Random Forest      | 0.915    | 0.773    | 0.915   |
| XGBoost            | 0.920    | 0.787    | 0.941   |

 1. **K-Nearest Neighbor (KNN):**

- Akurasi cukup tinggi, namun bukan yang terbaik karena lebih rendah dari random forest dan XGBoost.
- F1 Score relatif rendah, mengindikasikan bahwa model kurang optimal dalam menangani ketidakseimbangan antara precision dan recallâ€”khususnya untuk memprediksi nasabah gagal bayar.
- ROC-AUC juga merupakan yang paling rendah dari ketiga model, menunjukkan bahwa kemampuan diskriminatifnya terhadap dua kelas (gagal vs tidak gagal bayar) masih kurang.

2. **Random Forest:**
- Akurasi meningkat menunjukan model cukup baik
- F1 Score lebih baik dibanding KNN, menunjukkan keseimbangan yang lebih baik dalam mengklasifikasikan kelas minoritas (nasabah gagal bayar).
- ROC-AUC juga tinggi, menandakan performa baik dalam membedakan kedua kelas.

3. **XGBoost:**
- Memiliki performa terbaik di semua metrik evluasi
- F1 Score tertinggi, menunjukkan Keseimbangan precision & recall sangat baik
- ROC-AUC tertinggi, menunjukan sangat baik dalam membedakan nasabah gagal bayar dan tidak

Berdasarkan data evaluasi tambahan untuk contoh data aktual (y_true = 1), prediksi masing-masing model adalah:

- K-Nearest Neighbor (KNN): 1

- Random Forest (RF): 1

- XGBoost (Boosting): 1

Dalam hal ini, ketiga model dapat memprediksi dengan benar bahwa pelanggan dalam sampel ini adalah gagal membayar pinjaman.

Secara keseluruhan, XGBoost adalah algoritma yang dapat dipilih untuk melakukan suatu prediksi karena, dapat memberikan hasil yang paling akurat.
"""